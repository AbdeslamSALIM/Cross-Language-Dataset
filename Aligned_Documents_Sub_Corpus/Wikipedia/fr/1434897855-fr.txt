 En mathématiques, les matrices servent à interpréter en termes calculatoires et donc opérationnels les résultats théoriques de l'algèbre linéaire et même de l'algèbre bilinéaire. Toutes les disciplines étudiant des phénomènes linéaires utilisent les matrices. Quant aux phénomènes non linéaires, on en donne souvent des approximations linéaires comme c'est le cas en optique géométrique avec les approximations de Gauss.DéfinitionsUne matrice à m'' lignes et ''n colonnes est un tableau rectangulaire de mn nombres, rangés ligne par ligne. Il y a m'' lignes, et dans chaque ligne ''n nombres.Passons maintenant à la définition formelle. Soient A un ensemble et (m,n) un couple d'entiers positifs. Le plus souvent, l'ensemble A est muni d'une structure de corps commutatif mais on utilise aussi fréquemment des matrices à coefficients dans un anneau.On appelle matrice à coefficients dans A'', de '''dimension' (ou taille) (m,n) (c'est-à-dire à m lignes et n colonnes), une famille (ai,j) d'éléments de A indexée par le produit cartésien des ensembles de nombres entiers 1,m et 1,n.La matrice M pourra être notée parM=(a_)_ou plus simplement (ai,j)i,j, voire (ai,j) si le contexte s'y prête.On représente généralement une matrice sous la forme d'un tableau rectangulaire. Par exemple, est représentée ci-dessous une matrice M'', à coefficients entiers, et de dimension ''(3,4) :M=\begin0 &1 & 2 & 3\\ 4 & 5 & 6 & 7\\ 8 & 9 & 10 & 11\\ \endDans cette représentation, le premier coefficient de la dimension est le nombre de lignes, et le deuxième, le nombre de colonnes du tableau. Une matrice pour laquelle le nombre m'' de lignes est égal au nombre ''n de colonnes sera dite matrice carrée de taille n''. Une matrice ne comportant qu'une seule ligne et ''n colonnes est appelée matrice ligne de taille n''. Une matrice ne comportant ''m lignes et une seule colonne est appelée matrice colonne de taille m.Pour repérer un coefficient d'une matrice, on indique son indice de ligne puis son indice de colonne, les lignes se comptant du haut vers le bas et les colonnes de la gauche vers la droite. Par exemple, on notera ai,j, les coefficients de la matrice M'', pour 1\le i\le 3 désignant le numéro de la ligne sur laquelle figure le coefficient envisagé, et 1\le j\le 4 désignant son numéro de colonne ; ainsi ''a2,4=7.La disposition générale des coefficients d'une matrice M'' de taille ''(m,n) est donc la suivanteM=\begina_ & a_ & \cdots & a_\\ a_ & a_ & \cdots & a_\\ \vdots & \vdots & \ddots & \vdots\\ a_ & a_ & \cdots & a_\\ \endPour effectuer certaines opérations, il peut être utile de travailler sur le système des lignes ou des colonnes d'une matrice. On pourra alors l'écrire sous une des formes suivantesM=\beginL_1\\L_2\\\vdots \\ L_m\\\end ou M = \begin C_1 & C_2 & \dots & C_n\\ \end.L'ensemble des matrices à coefficients dans A'' possédant ''m lignes et n colonnes est noté  M_(A) (ou parfois  M(m,n,A)).Lorsque ''m=n on note plus simplement  M_n(A).Soit  M= (a_)_ \in M_(A), on appelle transposée de M la matrice ^tM=(a_)_. Remarquons que ^tM\in M_(A).Par exemple, avec la matrice M des exemples précédents, on a^tM=\begin 0 & 4 & 8\\ 1 & 5 & 9\\ 2 & 6 & 10\\ 3 & 7 & 11\\ \end L'opération de transposition est involutive, c'est-à-dire que ^t(^t\!M)=M.Espaces de matricesOn suppose maintenant que A'' est muni d'une structure d'anneau unitaire ; les éléments de ''A seront appelés scalaires, par opposition aux matrices dont nous allons voir qu'elles peuvent être considérées comme des vecteurs.Addition et multiplication par un scalaireOn définit sur  M_(A) une loi de composition interne provenant de l'addition des scalaires : (a_)+(b_)=(c_)  ou \displaystyle c_=a_+b_ On ne peut additionner que deux matrices de même taille.Exemple :\begin \begin 0 &1 & 2 & 3\\ 4 & 5 & 6 & 7\\ \end + \begin 0 &0 & 1 & 1\\ 0 & 1 & 0 & 1\\ \end = \begin 0 &1 & 3 & 4\\ 4 & 6 & 6 & 8\\ \end \end Pour chaque valeur du couple (m,n), l'espace  M_(A) devient alors un groupe abélien, d'élément neutre la matrice nulle, celle dont tous les coefficients valent 0.On définit aussi une opération à gauche de A'' sur chaque espace  M_(A)en associant à chaque matrice (a_) à coefficients dans ''A et chaque scalaire  \lambda  dans A'', la matrice  \lambda (a_)=(\lambda a_) obtenue en effectuant la multiplication, dans ''A, de tous les coefficients de la matrice initiale par \lambda : c'est la multiplication par un scalaire.En reprenant toujours la matrice M du premier exemple :2M=\begin0 &2 & 4 & 6\\ 8 & 10 & 12 & 14\\ 16 & 18 & 20 & 22\\ \end Les espaces  M_(A) ainsi obtenus ont donc une structure de A''-module à gauche, et plus particulièrement de ''A-espace vectoriel, si A est un corps commutatif.Base canonique de l'espace des matricesAlors  M_(A) est un  A -module libre de dimension mn , muni d'une base canonique  (E_)_. La matrice E_ est celle dont tous les coefficients sont nuls sauf celui d'indice  (i,j) , qui vaut 1.Pour toute matrice M, les coordonnées dans la base canonique sont les coefficients M = \sum _ a_ E_ Exemple :\begin 0 &1 & 2 \\ 4 & 3 & 1 \\ \end = 0\cdot \begin 1 &0 & 0 \\ 0 & 0 & 0 \\ \end + 1\cdot \begin 0 &1 & 0 \\ 0 & 0 & 0 \\ \end + 2\cdot \begin 0 &0 & 1 \\ 0 & 0 & 0 \\ \end + 4\cdot \begin 0 &0 & 0 \\ 1 & 0 & 0 \\ \end + 3\cdot \begin 0 &0 & 0 \\ 0 & 1 & 0 \\ \end + 1\cdot \begin 0 &0 & 0 \\ 0 & 0 & 1 \\ \end Produit matricielOn commence par définir le produit d'une matrice ligne par une matrice colonne. Soit n'' un nombre entier, ''L une matrice ligne, xi ses coefficients, C'' une matrice colonne, ''yi ses coefficients. On les suppose toutes deux de taille n''. On définit alors le produit, considéré comme un scalaire ou une matrice de dimension ''(1,1) :  LC = \beginx_1&\dots&x_n\end\beginy_1\\\vdots\\ y_n\end=\sum_ x_iy_iOn remarque la condition de compatibilité sur les tailles des matrices (égalité du nombre de colonnes de la première avec le nombre de lignes de la deuxième). On définit maintenant plus généralement un produit entre deux matrices, la première, (xi,j) dans  M_(A), la deuxième, (yi,j) dans  M_(A), toujours avec une condition de compatibilité sur les tailles (et l'ordre des facteurs de la multiplication ne peut en général pas être changé). Le résultat obtenu est une matrice de  M_(A), dont les coefficients (zi,j) sont obtenus par :z_=\sum_ x_y_=x_y_+x_y_+\cdots+ x_y_À la lumière de l'exemple de la multiplication d'une matrice ligne par une matrice colonne, on peut reformuler cette définition en disant que ce coefficient est égal au produit de la ligne i'' de la première matrice par la colonne ''j de la deuxième, ce qui s'écrit de la manière suivante, si les Li sont les lignes de la première matrice, et les Cj les colonnes de la deuxième, le produit est :  \beginL_1\\\vdots\\ L_m\end\beginC_1&\dots&C_p\end=\begin L_1C_1 & L_1C_2 & \dots & L_1C_p\\L_2C_1 & L_2C_2 & \dots & L_2C_p\\ \vdots & \vdots & & \vdots\\ L_mC_1 & L_mC_2 & \dots & L_mC_p\\\end.Pour calculer en pratique un produit, il est nécessaire de visualiser l'opération. On considère le coefficient c_ de la matrice produit MN si M'' est une matrice de type (4, 2), et ''N est une matrice de type (2, 3).c_ = \sum_^2 a_b_ = a_b_+a_b_Le produit matriciel est associatif, distributif à droite et à gauche par rapport à l'addition matricielle. En revanche, même si les dimensions permettent de donner un sens à la question, même si l'anneau des scalaires est commutatif, un produit de matrices ne commute en général pas : MN n'est pas égal à NM, par exemple : M =\begin
    0 &amp; 0 \\ 
    1 &amp; 0
 \end\quad N =\begin 
   1 &amp; 0 \\ 
   0 &amp; 0 \\ 
     \end\quad MN =\begin 
   0 &amp; 0 \\ 
   1 &amp; 0 \\ 
     \end\quad NM =\begin 
   0 &amp; 0 \\ 
   0 &amp; 0 \\ 
     \end Remarque : le produit de deux matrices non nulles est peut être nul, comme l'exemple au dessus.Ce contre-exemple prouve même que les matrices MN et NM ne sont pas toujours semblables.Lorsque l'anneau des scalaires est commutatif, la transposition et le produit matriciel vérifient la propriété :\forall M\in M_(A),\ \forall N\in M_(A),\ ^t(MN)=^tN\,^t\!MMatrice identité et inverse d'une matricePour chaque nombre entier n'', on note ''In la matrice carrée de taille n'' dont les coefficients diagonaux sont égaux à 1 et dont les autres coefficients sont nuls ; elle est appelée '''matrice identité' de taille n. I_1=1\quadI_2= \begin 1 & 0\\ 0 & 1\end \quad I_3=\begin 1 & 0 & 0\\ 0 & 1 & 0 \\0 & 0 & 1 \end \quad I_n=(\delta_)_ où \delta_ désigne le symbole de Kronecker.Sous réserve de compatibilité des tailles, les matrices In sont neutres à droite et à gauche pour la multiplication. \forall M\in M_(A),\ I_m\,M=M\,I_n =MSoit M'' une matrice de dimension ''(m,n). On dit que M'' est 'inversible' à droite (respectivement à gauche) si et seulement s'il existe une matrice ''N de taille (n,m) telle que \displaystyle MN = I_m  (respectivement \displaystyle NM = I_n ). Pour une matrice carrée, à coefficients dans un anneau commutatif, être inversible à droite et à gauche sont deux propriétés équivalentes. Une matrice les vérifiant sera dite inversible, sans plus de précision. Le déterminant est une fonction importante sur les matrices carrées, qui permet notamment de tester leur inversibilité. Le sous-ensemble de  M_n(A) constitué des matrices inversibles possède une structure de groupe pour le produit matriciel, est appelé groupe linéaire et noté \displaystyle GL_n(A).Algèbre des matrices carréesLorsque l'anneau A est commutatif, l'ensemble des matrices carrées  M_n(A) est donc muni d'une structure d'algèbre associative et unitaire avec l'addition matricielle, le produit par un scalaire et le produit matriciel.On appelle matrice scalaire une matrice de la forme  aI_n où a est un élement de l'anneau  K. aI_n=\begina & 0 & \dots & 0\\ 0 & a & \dots & 0\\ \vdots &\vdots&\ddots& 0\\ 0 & 0&\dots & a \end Ces matrices s'appellent matrices scalaires car elles se comportent comme des scalaires, vis-à-vis de la multiplication : \forall a\in A,\ \forall M\in M_n(A),\ (aI_n)M = a\cdot MLorsque  A est commutatif, ou à défaut, losque  a est central dans  A, c'est-à-dire lorsque  a commute avec tous les éléments de  A, on a \forall a\in A,\ \forall M\in M_n(A),\ (aI_n)M = M(aI_n) = a\cdot MRéciproquement, toute matrice  N de  M_n(A) telle que \forall M\in M_n(A),\ MN = NM  est une matrice scalaire  aI_n où  a est central dans  M. Ceci se démontre en prenant pour M'' les matrices de la base canonique.Une matrice de la forme : \begina_1 & 0 & \dots & 0\\ 0 & a_2 & \dots & 0\\ \vdots &\vdots&\ddots& 0\\ 0 & 0&\dots & a_n \end sera dite matrice diagonale.Outre le déterminant, une autre fonction à noter est la trace. Toutes deux apparaissent dans un objet plus général, le polynôme caractéristique, qui à son tour permet d'obtenir certaines caractérisations des matrices diagonalisables (c'est-à-dire semblable — voir plus bas — à une matrice diagonale), ou de la trigonalisation.Actions du groupe linéaireIl existe plusieurs manières de faire agir le groupe linéaire \displaystyle GL_n(A) sur les espaces de matrices, et notamment :action par multiplication à gauche de \displaystyle GL_m(A) sur \displaystyle M_(A), qui à P'' et ''M, associe PM,action (à droite) par multiplication à doite de \displaystyle GL_n(A) sur \displaystyle M_(A), qui à Q\in\displaystyle GL_n(A) et M\in M_(A), associe MQ,action par conjugaison de \displaystyle GL_n(A) sur M_n(A), qui à X\in\displaystyle GL_n(A) et M\in M_n(A), associe XMX^.On décrit maintenant les résultats classiques sur ces actions, lorsque les scalaires forment un corps commutatif. Les deux premières actions sont souvent considérées simultanément ; on s'intéresse donc à la question : deux matrices M1 et M2 de dimension (m,n) étant données, existe-t-il des matrices P\in\displaystyle GL_m(A) et Q\in\displaystyle GL_n(A) telles que M_1=PM_2Q ? Si tel est le cas, les deux matrices M1 et M2 sont dites équivalentes. Le résultat principal est que deux matrices sont équivalentes si et seulement si elles ont même rang, ce qui s'exprime encore en disant que le rang est un invariant complet pour les doubles classes définies par les deux actions de multiplication à gauche et à droite. Par ailleurs, une matrice étant donnée, on peut trouver d'autres matrices privilégiées (les matrices échelonnées) dans la même orbite pour une de ces actions par la méthode du pivot de Gauss.Pour l'action par conjugaison, deux matrices carrées M1 et M2 de taille n'' dans la même orbite admettent une relation de la forme M_1=PM_2P^, pour une certaine matrice ''P inversible de taille n'' ; deux telles matrices sont dites '''semblables'. La description d'un système complet d'invariants est plus délicate. On appelle ces invariants les invariants de similitude. D'un point de vue algorithmique, la réduction d'une matrice quelconque à une matrice sous une forme privilégiée se fait par un algorithme inspiré de celui du pivot de Gauss, voir théorème des facteurs invariants.Interprétations linéairesUn intérêt principal des matrices est qu'elles permettent d'écrire commodément les opérations habituelles de l'algèbre linéaire, avec une certaine canonicité.CoordonnéesLe premier point est de remarquer que le A''-module ''An s'identifie canoniquement à l'espace de matrices colonnes M_(A) : si ei est le n''-uplet dont tous les coefficients sont nuls, sauf le ''i-ème qui vaut 1, on lui associe la matrice colonne élémentaire dont tous les coefficients sont nuls sauf le i''-ème qui vaut 1, et on étend l'identification par linéarité ; la matrice associée à chaque ''n-uplet sera appelée matrice coordonnée canonique.D'autres identifications sont cependant possibles ; lorsqu'on peut parler de base (si l'anneau des scalaires est un corps, ou est principal, par exemple), on peut associer les matrices colonnes élémentaires à n'importe quelle base de l'espace An (ou plus généralement d'un A-module libre), puis à nouveau étendre par linéarité ; les matrices associées seront appelées matrices coordonnées dans la base envisagée.On peut concaténer les matrices coordonnées, dans une base fixée, de plusieurs n''-uplets. On obtient ainsi la matrice coordonnée d'une famille de vecteurs. Le ''rang de la matrice est alors défini comme la dimension de la famille de ces vecteurs. En particulier la matrice d'une base dans une autre base est appelée matrice de passage entre ces deux bases, ou matrice de changement de base. Si X'' et ''X' '' sont les matrices coordonnées du même vecteur dans deux bases ''B et C'', et que ''P est la matrice de la base C'' dans la base ''B, on a la relation (une matrice de passage est toujours inversible) : X=PX'\quad X'=P^ X Applications linéairesSoient  E et  F  deux  A-modules (à gauche) libres de dimensions finies. Soit B=(e1,...,ep) une base de  E et C=(f1,...,fn) une base de  F. Soit enfin \phi une application linéaire de  E dans F.On appelle matrice de \phi dans le couple de bases (B,C) la matrice :  mat_\,\varphi = mat_(\varphi(e_1),\dots,\varphi(e_p)). L'application de  \mathcal L(E,F) dans  M_(A) qui à  \varphi associe la matrice : M=mat_\, \varphi est un isomorphisme.Soit x'' un vecteur de ''E. Notons  y=\varphi(x),  X=mat_\,x,  Y=mat_\,y et  M=mat_\,\varphi. Lorsque l'anneau A est commutatif (ou si l'on travaille avec des modules à droite), ces matrices sont reliées par : \displaystyle Y=MX Cette formule devient  ^tY=^t\left(M\,X\right)=^t\!\!X\,^tM pour des modules à gauche sur un anneau non commutatif.L'application X\mapsto MX  du A-module  M_(A) dans le A-module  M_(A)  est linéaire et sa matrice dans les bases canoniques est \displaystyle M. C'est un point clef du lien entre algèbre linéaire et matrices.En conséquence, il arrive souvent que l'on l'identifie avec l'application linéaire ci-dessus. On parlera alors de noyau de la matrice, d'espaces propres de la matrice, d'image de la matrice, etc.Si  E et  F sont deux  A-modules, B'' et ''B' '' deux bases de ''E, de cardinal p'', et ''C et C' '' deux bases de ''F de cardinal n'', ''P la matrice de B' '' dans ''B et Q'' la matrice de ''C' '' dans ''C, alors les deux matrices M'' et ''M' '' d'une même application linéaire  \varphi de  E dans  F, dans les couples de bases ''(B,C) et (B',C') sont liées par la relation : M'=Q^ M P\quad M=Q M' P^ .On constate ainsi que deux matrices équivalentes, d'après la définition donnée plus haut dans l'article, sont deux matrices qui représentent la même application linéaire dans des bases différentes.En particulier, dans le cas d'un endomorphisme, si on impose ''B= B' '' et ''C=C' '', les formules précédentes se simplifient : M'=P^ M P\quad M=P M' P^ On constate ainsi que deux matrices semblables, d'après la définition donnée plus haut dans l'article, sont deux matrices qui représentent le même endomorphisme dans des bases différentes.Interprétations bilinéairesDans ce paragraphe, l'anneau commutatif des scalaires sera noté (K,+,\cdot). Dans la plupart des applications, ce sera un corps commutatif.'' Le cas non commutatif existe aussi mais il faut prendre quelques précautions et les notations deviennent trop lourdes pour cet article.''Matrice d'une forme bilinéaireSoit  E un  K-module libre et  \mathcal B=(e_1,\dots,e_n) une base de  E.Soit  f : E\times E \to K une forme bilinéaire. On définit la matrice de  f  dans la base  \mathcal B par la formule suivante :mat_\, f = (f(e_i,e_j))_=\begin f(e_1,e_1) & f(e_1,e_2) & \dots & f(e_1,e_n)\\ f(e_2,e_1) & f(e_2,e_2) & \dots & f(e_2,e_n)\\ \vdots & \vdots & \ddots & \vdots\\ f(e_n,e_1) & f(e_n,e_2) & \dots & f(e_n,e_n)\\ \end Dans le cas particulier où  K=\R et  f  est un produit scalaire, cette matrice est appelée matrice de Gram.mat_\, f est symétrique (respectivement antisymétrique) si et seulement si  f  est symétrique (respectivement antisymétrique).Soit  x et  y  deux vecteurs de  E. Notons  X et  Y leurs coordonnées dans la base  \mathcal B et A=mat_\, f. On a alors la formule :  f(x,y) =^tXAY.Deux formes bilinéaires sont égales si et seulement si elles ont la même matrice dans une base donnée.Matrice d'une forme quadratiqueLorsque (K,+,\cdot) est un corps de caractéristique différente de 2, on appelle matrice d'une forme quadratique la matrice de la forme bilinéaire symétrique dont est issue la forme quadratique.Formule de changement de baseSoit  E un  K-module libre,  \mathcal B et  \mathcal C deux bases de  E. Soit  f : E\times E \to K une forme bilinéaire.Notons A = mat_\, f la matrice de  f dans la base  \mathcal B et B = mat_\, f la matrice de  f dans la base  \mathcal C. Notons  P =mat_\, \mathcal C la matrice de passage. On a alors : \displaystyle B=^tPAPMatrices congruentesDeux matrices carrées A et B sont dites congruentes s'il existe une matrice inversible P telle que  A = ^tPBP.Deux matrices congruentes sont deux matrices qui représentent la même forme bilinéaire dans deux bases différentes.Lorsque (K,+,\cdot) est un corps de caractéristique différente de 2, toute matrice symétrique est congruente à une matrice diagonale. L'algorithme utilisé s'appelle réduction de Gauss à ne pas confondre avec le pivot de Gauss.Matrices orthogonalesMatrices unitairesMatrices symétriquesMatrices antisymétriquesMatrice d'une forme sesquilinéaireMatrices hermitiennesUne matrice A est dite hermitienne si A_ = \overline_ \quad \forall \ i,j.ExempleA=\begin3&i&-5i\\-i&-2&5\\ 5i&5&10\end est une matrice hermitienne.Décomposition d'une matriceOn utilise abusivement le terme décomposition d'une matrice, qu'il s'agisse d'une véritable décomposition (en somme) comme dans la décomposition de Dunford ou d'une factorisation comme dans la plupart des autres décompositions.Réduction d'une matrice carrée    Réduire une matrice, c'est trouver une matrice qui lui est semblable le plus simple possible.Une matrice diagonalisable est une matrice semblable à une matrice diagonale. A  est diagonalisable si et seulement s'il existe une matrice inversible  P  et une matrice diagonale  D  telles que  A=P^DP .Sur un corps algébriquement clos, on dispose de la réduction de Jordan qui est optimale et il existe des décompositions intermédiaires comme la décomposition de Dunford qui utilise les sous-espaces caractéristiques ou celle de Froebenius qui utilise les sous-espaces cycliques.Les polynômes d'endomorphismes jouent un rôle crucial dans les techniques de réduction.Décomposition LUC'est une factorisation en produit de deux matrices triangulaires.En lien avec le pivot de Gauss, c'est une méthode qui permet d'inverser une matrice.Décomposition QRC'est un résultat sur les matrices à coefficients réels ou à coefficients complexes.C'est une factorisation en produit d'une matrice orthogonale et d'une matrice triangulaire.C'est une traduction matricielle du procédé de Gram-Schmidt.Décomposition polaireC'est un résultat sur les matrices à coefficients réels ou à coefficients complexes.C'est une factorisation en produit d'une matrice orthogonale et d'une matrice symétrique strictement positive dans le cas réel, en produit d'une matrice unitaire et d'une matrice hermitienne strictement positive dans le cas complexe.On peut décomposer à droite ou à gauche.On a unicité de la factorisation pour les matrices inversibles.Normes et rayon spectralDans tout ce paragraphe, les matrices considérées sont dans \mathcal M_n(\R)  ou \mathcal M_n(\mathbb C). De plus on identifie une matrice  A  avec l'endomorphisme de \mathcal M_(\R)  ou \mathcal M_(\mathbb C) qui à la matrice colonne  X  associe la matrice colonne  AX . Le cas réel et le cas complexe sont identiques.Normes et normes d'algèbreSoit  N  une norme sur \mathcal M_n(\R)  ou \mathcal M_n(\mathbb C).On dira que  N  est une norme d'algèbre (on dit aussi norme de Banach) si et seulement si \forall (A,B),\ N(AB)\le N(A)N(B)Certains auteurs imposent en outre que  N(I_n)=1Pour une norme quelconque, l'application bilinéaire  (A,B) \mapsto AB  étant continue (on est en dimension finie), on est assuré de l'existence d'une constante  k0  telle que \forall (A,B),\ N(AB)\le kN(A)N(B)Par suite, la norme \frac1k N est une norme d'algèbre. Toute norme est donc proportionnelle à une norme d'algèbre.Rayon spectralSoit  A  une matrice carrée à coefficients complexes. On appelle rayon spectral le plus grand module des valeurs propres de  A . Dans tout ce qui suit, on notera  \rho(A) le rayon spectral de  A .Théorème : Pour toute norme d'algèbre  N  sur \mathcal M_n(\R)  (respectivement dans \mathcal M_n(\mathbb C)) et pour toute matrice  A  dans \mathcal M_n(\R)  (respectivement dans \mathcal M_n(\mathbb C)), l'inégalité suivante est vérifiée : \rho(A)\le N(A)Démonstration : Soit  \lambda  une valeur propre de  A  et  X  un vecteur propre associé. notons  B  la matrice carrée dont la première colonne est  X  et les autres sont nulles. On a  AB = \lambda B donc  N(AB) = |\lambda| N(B)\le N(A)N(B) et on peut simplifier par  N(B) car le vecteur  X  étant non nul, il en est de même de la matrice  B.De plus, on montre que  \rho(A) = \inf N(A) , la borne inférieure étant prise sur l'ensemble des normes subordonnées, donc a fortiori sur l'ensemble des normes d'algèbre.Par contre, l'égalité peut s'avérer impossible. Il suffit pour cela de considérer une matrice non nulle dont le rayon spectral est nul :  \begin 0 & 0\\ 1 & 0 \end Normes subordonnées  Lorsqu'on munit \mathcal M_(\R)  d'une norme \|.\|, on munit automatiquement \mathcal M_n(\R)  d'une norme, appelée norme subordonnée à \|.\|. Elle est donnée par la formule suivante donnée pour une matrice  A  quelconque dans \mathcal M_(\R)  :  |||A ||| = \sup _ \|AX\| En notant  A=(a_)_ , on aSi  \|X\| = \max_ |x_i| , (norme infinie), alors la norme de  A  vaut : \max_ \sum _ |a_|Si  \|X\| = \sum _ |x_i| , (norme indice 1), alors la norme de  A  vaut : \max_ \sum _ |a_|Toute norme subordonnée est une norme d'algèbre avec en plus  |||I_n |||=1.Démontrer la réciproque ou insérer contre-exemple pour la réciproque. Pour la réciproque, on pourrait construire  \|X\| = \inf_ N(A) mais je n'ai pas vérifié que ça marche.D'autre part, si deux normes subordonnées sont égales, les normes auxquelles celle-ci sont subordonnées sont proportionnelles. C'est une conséquence immédiate du théorème de Hahn-Banach.Norme subordonnée à la norme euclidienneOn se place dans la cas où \mathcal M_(\R)  est muni de sa norme euclidienne canonique donnée par  \|X\|^2 = \sum _ |x_i|^2.Lorsque  A  est une matrice symétrique (respectivement hermitienne), la norme de  A  est égale au rayon spectral de  A .Dans le cas où  A  est une matrice quelconque, la norme de  A  est égal à  \sqrt .La norme de  A  est donc la plus grande des valeurs singulière de  A  (les valeurs singulière de  A  sont, par définition, les racines carrées des valeurs propres de ^tAA).Exponentielle d'une matriceSoit  A\in\mathcal M_n(\mathbb C) , Soit  N  une norme d'algèbre et \sum a_n z^n  une série entière de rayon de convergence  R .Alors si  N(A) , la série \sum a_n A^n  est absolument convergente. La ruse, c'est que N(A^n)\le N(A)^n.En particulier, on peut définir, pour toute matrice carrée complexe, la quantité \exp (A) = \sum_^ \frac1 A^nLe calcul effectif de cette exponentielle se fait par réduction de la matrice.L'exponentielle joue un rôle central dans l'étude des systèmes linéaires d'équations différentielles.Voir aussiArticles connexesThéorie des matricesMatrices semblablesMatrices équivalentesMatrice nilpotenteMatrice de transitionMatrice aléatoireMatrice triangularisableMatrice diagonalisableMatrice de rotationComatriceConditionnement (analyse numérique)TenseurReprésentation des groupesProduit matricielPaires de matrices commutantesBibliographieJ.M. Arnaudiès et H. Fraysse Cours de mathématiques, Dunod, 1980Bourbaki Algèbre I : Chapites 1 à 3, CCLS, 1970.Bourbaki Algèbre I : Chapites 1 à 3, Springer, 2006.Liens externesLes matrices : formes de représentation et pratiques opératoires (1850-1930), par Frédéric Brechenmacher  Catégorie:Matricear:مصفوفة az:Matris bg:Матрица (математика) bn:মেট্রিক্স bs:Matrica (matematika) ca:Matriu (matemàtiques) cs:Matice da:Matrix de:Matrix (Mathematik) el:Πίνακας (μαθηματικά) en:Matrix (mathematics) eo:Matrico es:Matriz (matemática) et:Maatriks fa:ماتریس (ریاضی) fi:Matriisi gl:Matriz (matemáticas) he:מטריצה hi:व्यूह hr:Matrica (matematika) hu:Mátrix (matematika) id:Matriks (matematika) is:Fylki (stærðfræði) it:Matrice ja:行列 ko:행렬 lo:ມາຕຣິກ lt:Matrica (matematika) mk:Матрица ml:മാട്രിക്സ് ms:Matriks (matematik) nl:Matrix (wiskunde) nn:Matrise no:Matrise pl:Macierz pt:Matriz (matemática) ro:Matrice (matematică) ru:Матрица (математика) scn:Matrici (matimàtica) simple:Matrix (mathematics) sk:Matica (matematika) sl:Matrika sq:Matrica sr:Матрица (математика) sv:Matris ta:அணி th:เมทริกซ์ (คณิตศาสตร์) tr:Dizey uk:Матриця (математика) ur:میٹرکس vi:Ma trận (toán học) zh:矩阵