L'analyse de la variance (terme souvent abrégé par le terme anglais ANOVA : '''AN'alysis 'O'f 'VA'riance'') est un test statistique permettant de vérifier que plusieurs échantillons sont issues d'une même population.Ce test s'applique lorsque que l'on mesure une ou plusieurs variables explicatives discrètes (appelées alors facteurs de variabilités, leurs différentes modalités étant appelées "niveaux") qui influence sur la distribution d'une variable continue à expliquer. On parle d'analyse à un facteur, lorsque l'analyse porte sur un modèle décrit par un facteur de variabilité, d'analyse à deux facteurs ou d'analyse multifactorielle.PrincipeNotons Y la variable à expliquer continue, i l'indice identifiant l'individu et X_ les facteurs de variabilités discrets.On suppose que les facteurs de variabilités x_ influencent uniquement les moyennes des distributions et que leurs variances restent identique (hypothèse d'homoscédasticité) et constantes : chaque facteur y_ suit une loi normale \mathcal(0, \sigma^2) .L'hypothèse nulle correspond au cas où les distributions suivent la même loi normale. L'hypothèse alternative est qu'il existe au moins une distribution dont la moyenne s'écarte des autres moyennes : =m_=...=m_=m \\ ~m_i \neq m_j \end.Exemple illustratifPrenons un exemple pour illustrer la méthode. Imaginons un agriculteur qui souhaite acheter de nouvelles vaches pour sa production laitière. Il possède trois races différentes de vaches et se pose donc de la question de savoir si la race est importante pour son choix. Il possède comme informations la race de chacune de ses bêtes (c'est la variable explicative discrète ou facteur de variabilité, qui peut prendre 3 valeurs différentes) et leurs productions de lait journalières (c'est la variable à expliquer continue, qui correspond au volume de lait en litre).Dans notre exemple, l'hypothèse nulle revient à considérer que toutes les vaches produisent la même quantité de lait journalière (au facteur aléatoire près) quelque soit la race. L'hypothèse alternative revient à considérer qu'une des races produit significativement plus ou moins de lait que les autres.Supposerons que les productions sont :Pour la race A : 20,1 ; 19,8 ; 21,3 et 20,7Pour la race B : 22,6 ; 24,1 ; 23,8 ; 22,5 ; 23,4 ; 24,5 et 22,9Pour la race C : 31,2 ; 31,6 ; 31,0 ; 32,1 et 31,4RemarquesL'idée de l'analyse de la variance repose sur un modèle qu'on se donne a priori des données. On s'attache ensuite à l'étude de la contribution de ces différents termes à la variance de Y, grâce à une décomposition dite de « l'analyse de la variance ».Il est important de comprendre que l'analyse de la variance n'est pas un test permettant de « classer » des moyennes par exemple. Comme on l'a noté précédemment, l'hypothèse nulle H0 revient à dire que toutes les moyennes sont égales. Le but ici est donc beaucoup plus « humble » : il s'agit de comparer des moyennes de différents groupes et de dire si, parmi l'ensemble, au moins une d'entre elles diffère des autres, mais on ne sait ni laquelle ni combien d'entre elles. Déterminer quel groupe a un effet différentiel, c’est-à-dire quel groupe présente une moyenne de la variable étudiée différente des autres, est un problème tout à fait différent. Il peut se poser après une ANOVA et les tests associés sont dits « tests de comparaison multiples », ou MCP pour Multiple Comparison Test. Ces tests obligent en général à augmenter les risques de l'analyse (en termes de risque statistique). Dans la biologie moderne, notamment, des tests MCP permettent de prendre en compte le risque de façon correcte malgré le grand nombre de tests effectués (par exemple pour l'analyse de biopuces). On pourra se reporter notamment aux procédures de Bonferonni et de Sidak.Il s'agit d'une généralisation à k populations du test T de Student de comparaison de moyennes de deux échantillons.Décomposition de la varianceLa première étape de l'analyse de la variance consiste à expliquer la variance totale à expliquer sur l'ensemble des échantillons en fonction de la variance due aux facteurs (la variance expliquée par le modèle), de la variance due à l'interaction entre les facteurs et de la variance résiduelle aléatoire (la variance non expliquée par le modèle). S_n^2 étant un estimateur biaisé de la variance, on utilise la somme des carrés des écarts (SCE en français, SS pour Sum Square en anglais) pour les calculs et l'estimateur non biaisé de la variance S_^2 (également appelé carré moyen ou CM).L'écart (sous entendu l'écart à la moyenne) d'une mesure est la différence entre cette mesure et la moyenne :.|border="0"La somme des carrés des écarts SCE et l'estimateur S_^2 se calculent à partir des formules :)^2 |border=0 ^2 = \frac |border=0Cette décomposition de la variance est toujours valable, même si les variables ne suivent pas de loi normales.Analyse de la variance à un facteurSoit un facteur de variabilité pouvant prendre les niveaux i = 1..p, n_i le nombre d'individu dans chaque niveau et n le nombre d'individu total. La variable à expliquer s'écrit y_i^j avec i = 1..p et j = 1..n_i.La variable à expliquer peut être modélisée par la relation :  ~ |border=1avec \alpha_i l'effet du niveau i du facteur et \epsilon_ l'erreur aléatoire (qui suit alors une loi normale \mathcal(0, \sigma^2) ).Dans ces conditions, on montre que la somme des carrés des écarts (et donc la variance) peut être calculer simplement par la formule :  = SCE_\text + SCE_\text ~ La part de la variance totale SCE_\text qui peut être expliquée par le modèle (SCE_\text, aussi appelée variabilité inter-classe, SSB ou Sum of Square Between class) et la part de la variance totale SCE_\text qui ne peut être expliquée par le modèle (SCE_\text aussi appelée variabilité aléatoire, variabilité intra-class, bruit, SSW ou Sum of Square Within class) sont données par les formules :  = \sum_^p n_i (\overline - \overline)^2 |border=0  = \sum_^p \sum_^ (y_i^j - \overline)^2 |border=0 = \sum_^p \sum_^ (y_i^j - \overline)^2.&nbsp;En décomposant ~ y_i^j - \overline = (y_i^j - \overline) + (\overline - \overline),&nbsp;on peut écrire ~ SCE_ = \sum_^p \sum_^ ((y_i^j - \overline) + (\overline - \overline))^2&nbsp; = \sum_^p \sum_^ (y_i^j - \overline)^2 + \sum_^p \sum_^ (\overline - \overline)^2 + \sum_^p \sum_^ 2( y_i^j - \overline).(\overline - \overline) .&nbsp;En remarquant que ~ \sum_^p \sum_^ ( y_i^j - \overline).(\overline - \overline) = \sum_^p (\overline \sum_^ (y_i^j - \overline) - \overline \sum_^ (y_i^j - \overline)) = 0 ,&nbsp;on peut écrire ~ SCE_ = \sum_^p \sum_^ (y_i^j - \overline)^2 + \sum_^p \sum_^ (\overline - \overline)^2 &nbsp; = \sum_^p \sum_^ (y_i^j - \overline)^2 + \sum_^p n_i (\overline - \overline)^2 &nbsp; = SCE_\text + SCE_\text ~.Analyse des résidusIl est toujours possible que le modèle ne soit pas correcte et qu'il existe un facteur de variabilité inconnu (ou supposé à priori inutile) qui ne soit pas intégré dans le modèle. Il est possible d'analyser la normalité de la distribution des résidus pour rechercher ce type de biais. Les résidus, dans le modèle, doivent suivre une loi normale \mathcal(0, \sigma^2)~). Tout écart significatif par rapport à cette loi normale peut être tester ou visualiser graphiquement : centerAnalyse de la variance à deux facteursSoit un premier facteur de variabilité pouvant prendre les niveaux i = 1..p, un second facteur de variabilité pouvant prendre les niveaux j = 1..q, n_ le nombre d'individu dans le niveau i du premier facteur et le niveau j du second facteur et n le nombre d'individu total. La variable à expliquer s'écrit y_ avec i = 1..p, j = 1..n_i et k = 1..m_j.La variable à expliquer peut être modélisée par la relation :  = \alpha_i + \beta_j + \gamma_ + \epsilon_ ~ |border=0avec \alpha_i l'effet du niveau i du premier facteur, \beta_j l'effet du niveau j du second facteur, \gamma_ l'effet d'interaction entre les deux facteurs et \epsilon_ l'erreur aléatoire (qui suit alors une loi normale \mathcal(0, \sigma^2)~).Le calcul présenté dans le cas à un facteur peut être transposé au cas à deux facteurs :  = SCE_\text + SCE_\text + SCE_\text + SCE_\text~ La part de la variance totale expliquée par le premier facteur (SCE_\text), la part de la variance totale expliquée par le second facteur (SCE_\text), l'interaction entre les deux facteurs (SCE_\text) et la part de la variance totale qui ne peut être expliquée par le modèle (SCE_\text, appelé aussi variabilité aléatoire ou bruit) sont données par les formules :SCE_\text = nq \sum_^p (\overline - \overline)^2 |border=0SCE_\text = np \sum_^q (\overline - \overline)^2 |border=0SCE_\text = n \sum_^p \sum_^q (\overline - \overline - \overline - \overline)^2 |border=0SCE_\text = \sum_^p \sum_^q \sum_^ (y_ - \overline)^2  |border=0L'analyse de l'interaction entre facteurs est relativement complexeVoir par exemple : Cours et TD de statistique de Lyon 1 pour un exemple d'analyse d'interaction dans un modèle à deux facteurs.. Dans le cas où les facteurs sont indépendants, on peut s'intéresser qu'aux effet principaux des facteurs. La formule devient alors :  = SCE_\text + SCE_\text + SCE_\text ~ |border=0Analyse de la variance multifactorielleOn peut encore décomposer la variance en ajoutant un terme pour chaque facteur et un terme pour chaque interaction possible :  \gamma_ + \epsilon_i |border=0 avec \alpha_j l'effet du jème facteur et \gamma_ l'interaction entre le jème et le kème facteur.L'analyse de la variance dans le cas de plusieurs facteurs de variabilité est relativement complexe : il est nécessaire de définir un modèle théorique correcte, étudier les interactions entre les facteurs, analyser la covariance.Test de Fisher dans l'analyse de la variancedegrés de liberté et variancesPuisque par hypothèse, la variable observée y_i suit une loi normale, les sommes des carrés des écarts suivent des lois du χ² (pour rappel, la loi du χ² de facteur k est définit comme étant la somme de k lois normales au carré) :SCE_\text = \sum_^p n_i (\overline - \overline)^2 \in \chi^2(DDL_\text) |border=0avec DDL_\text = \sum_^p 1 = p - 1 et p le nombre de niveaux du facteur de variabilité, et |border=0SCE_\text = \sum_^p \sum_^ (y_i^j - \overline)^2 \in \chi^2(DDL_\text) |border=0avec DDL_\text = \sum_^p (n_i - 1) = (n_1-1)+(n_2-1)+\cdots+(n_p-1) = n - p et n le nombre total d'individu.|border=0C'est-à-dire que la somme des carrés des écarts totale se décompose exactement en une somme de carrés d'écarts inter (en anglais between) ou encore factorielle, et une somme de carrés d'écarts intra (en anglais within) ou encore résiduelle. Notons bien que ce n'est pas la variance à proprement parler qui est décomposée. On obtiendra les différentes variances, ou carrés moyens, en divisant les sommes de carrés d'écart par leurs degrés de liberté.En effet, aux sommes de carrés d'écart, on associe les degrés de liberté suivants :Totale : nm-1Inter : m-1Intra : m(n-1)Et l'on vérifie que m-1+m(n-1) = nm-1. Les degrés de liberté se décomposent de manière additive comme les sommes de carrés d'écart. Pour les amateurs de géométrie vectorielle, la décomposition des degrés de liberté correspond à la décomposition d'un espace vectoriel de dimension nm en sous espaces supplémentaires et orthogonaux de dimensions respectives m-1 et m(n-1). Voir par exemple le cours dispensé par Toulouse III : http://www.univ-tlse1.fr/GREMAQ/Statistique/Eveweb/slimodlin.pdf pages 8 et 9. On peut se reporter aussi au livre classique de Scheffé (1959) On obtient alors les carrés moyens (mean squares) ou variances par les formules suivantes :CM_=\fracCM_=\fracCM_=\fractest d'adéquation à la loi de FisherF = \frac    Il se trouve (comme on peut le voir dans la décomposition mathématique) que les deux termes sont tous les deux une estimation de la variabilité résiduelle si le facteur A n'a pas d'effet. De plus, ces deux termes suivent chacun une loi de χ², leur rapport suit donc une loi de F (voir plus loin pour les degrés de liberté de ces lois). Résumons :Si le facteur A n'a pas d'effet, le rapport de S_ et S_ suit une loi de F et il est possible de vérifier si la valeur du rapport est « étonnante » pour une loi de FSi le facteur A a un effet, le terme S_ n'est plus une estimation de la variabilité résiduelle et le rapport \fracne suit plus une loi de F. On peut comparer la valeur du rapport à la valeur attendue pour une loi de F et voir, là aussi, à quel point le résultat est « étonnant ».Résumer les choses ainsi permet de clarifier l'idée mais renverse la démarche : on obtient en pratique une valeur du rapport \frac qu'on compare à une loi de F, en se donnant un risque α (voir l'article sur les tests et leurs risques). Si la valeur obtenue est trop grande, on en déduit que le rapport ne suit vraisemblablement pas une loi de F et que le facteur A a un effet. On conclut donc à une différence des moyennes.CM_ est l'estimateur S_A présenté au paragraphe précédent (première approche technique) et CM_ l'estimateur S_. On en déduit le F de Fisher, dont la distribution est connue et tabulée sous les hypothèses suivantes :Les résidus \epsilon sont distribués normalementAvec une espérance nulleAvec une variance \sigma^2 indépendante de la catégorie iAvec une covariance nulle deux à deux (indépendance)Le respect de ces hypothèses assure la validité du test d'analyse de la variance. On les vérifie a posteriori par diverses méthodes (tests de normalité, examen visuel de l'histogramme des résidus, examen du graphique des résidus en fonction des estimées) voir condition d'utilisation ci-dessous.Analyse de la variance à un facteurExemple illustratifAnalyse réalisée avec R :&gt; produc &lt;- c(20.1, 19.8, 21.3, 20.7, 22.6, 24.1, 23.8, 22.5, 23.4, 
24.5, 22.9, 31.2, 31.6, 31.0, 32.1, 31.4)
&gt; race &lt;- as.factor(c(&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, 
&quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;))
&gt; anova(lm(produc~race))
Analysis of variance TableResponse: produc
             Df     Sum Sq    Mean Sq    F value       Pr(&gt;F)    
race          2    307.918    153.959     357.44    4.338e-12 ***
Residuals    13      5.600      0.431                      
---
Signif. codes:    0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Analyse de la variance à deux facteursExemple illustratifNotre exploitant laitier souhaite améliorer la puissance de son analyse en augmentant la taille de son étude. Pour cela, il inclut les données provenant d'une autre exploitation. Les chiffres qui lui sont fournit sont les suivant :Pour la race A : 22,8 ; 21,7 ; 23,3 ; 23,1 ; 24,1 ; 22,3 et 22,7Pour la race B : 23,1 ; 22,9 ; 21,9 ; 23,4 et 23,0Pour la race C : 31,7 ; 33,1 ; 32,5 ; 35,1 ; 32,2 et 32,6Analyse réalisée avec R :&gt; produc &lt;- c(20.1, 19.8, 21.3, 20.7, 22.6, 24.1, 23.8, 22.5, 23.4, 
24.5, 22.9, 31.2, 31.6, 31.0, 32.1, 31.4, 22.8, 21.7, 23.3, 23.1, 
24.1, 22.3, 22.7, 23.1, 22.9, 21.9, 23.4, 23.0, 31.7, 33.1, 32.5, 
35.1, 32.2, 32.6)&gt; race &lt;- as.factor(c(&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, 
&quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, 
&quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;))&gt; centre &lt;- as.factor(c(rep(&quot;premier&quot;, 16), rep(&quot;second&quot;, 18)))&gt; anova(lm(produc~race*centre))
Analysis of variance TableResponse: produc
               Df    Sum Sq    Mean Sq     F value       Pr(&gt;F)    
race            2    696.48     348.24    559.6811    &lt; 2.2e-16 ***
centre          1      8.46       8.46     13.6012    0.0009636 ***
race:centre     2     12.23       6.11      9.8267    0.0005847 ***
Residuals      28     17.42       0.62                       
---
Signif. codes:    0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Limites d'utilisation de l'analyse de la varianceNormalité des distributionsLa décomposition de la variance est toujours valable, quelle que soit la distribution des variables étudiées. Cependant, lorsqu'on réalise le test de Fisher, on fait l'hypothèse de la normalité de ces distributions. Si les distributions s'écartent légèrement de la normalité, l'analyse de la variance est assez robuste pour être utiliser. Dans le cas où les distributions s'écartent fortement de la normalité, on pourra effectuer un changement de variables (par exemple, en prenant les variables y'_i = log(y_i)~ ou y''_i = y_i^2) ou utiliser un équivalent non paramétrique de l'analyse de la variance.HomoscédasticitéA l'opposé, l'ANOVA fait une autre hypothèse très forte et moins évidente. Il est en effet nécessaire que la variance dans les différents groupes soit la même. C'est l'hypothèse d'homoscedasticité. L'ANOVA y est très sensible. Il est donc nécessaire de la tester avant toute utilisation.Contrairement à ce que le nom de cette méthode laisse penser, celle-ci ne permet pas d'analyser la variance de la variable à expliquer mais de comparer les moyennes des distributions de la variable à expliquer en fonction des variables explicatives.Approches non paramétriquesLorsque les pré-supposés de l'ANOVA ne sont pas respectés (homoscédasticité par exemple), on entend souvent dire qu'il peut être plus judicieux d'utiliser l'équivalent non-paramétrique de l'ANOVA: le test de Kruskal Wallis pour le cas à un facteur ou, pour le cas à deux facteurs sans répétition, le test de Friedman. Pourtant, ces tests ne regardent pas la même chose. Comme il est écrit plus haut, l'ANOVA permet de comparer une mesure univariée entre des échantillons d'au moins deux populations statistiques. Le test de Kruskal-Wallis a pour hypothèse nulle l'homogénéité stochastique, c'est-à-dire que chaque population statistique est égale stochastiquement (on peut dire 'aléatoirement' pour simplifier) à une combinaison des autres populations. Ce test s'intéresse donc à la distribution contrairement à l'ANOVA et ne peut donc pas être considéré comme un équivalent au sens strict.Voir aussiTest (statistique)SourcesSCHERRER, B. (1984). Comparaison des moyennes de plusieurs échantillons indépendants. Tiré de "Biostatistiques". Gaëtan Morin Editeur. pp 422-463.RUXTON, G.D. & BEAUCHAMP, G. (2008). Some suggestions about appropriate use of the Kruskal-Wallis test.'' Animal Behaviour'' 76, 1083-1087.RéférencesCatégorie:Statistiquesbg:Дисперсионен анализ cs:Analýza rozptylu de:Varianzanalyse en:Analysis of variance es:Análisis de varianza gl:Análise da varianza hu:Varianciaanalízis id:Analisis varians it:Analisi della varianza ja:分散分析 ko:분산분석 lv:Dispersiju analīze nl:Variantie-analyse pl:Analiza wariancji sl:Analiza variance su:Analisis varian tr:Varyans analizi zh:方差分析