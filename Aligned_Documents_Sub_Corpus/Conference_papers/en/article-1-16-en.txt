Noisy data sets. In order to simulate the presence of class-noise in the breast-w data set, we add uniform noise in the class attribute using the AddNoise function of WEKA [Witten and Frank, 2005] – with various ratio: 20% and 50% amount of noisy class labels. We then proceed the train-test experiments on each artificially noisy data set. For each amount of noise (see in figure 6), classical extractors (frequentconfident rules and emerging patterns miners) succeed in outputing a set of “potentially” interesting patterns – notice that less rules arise from the most noisy contexts. However, once again the train-test experiments show the instability of classical measures. Moreover, the instability is emphasized in noisy contexts; indeed, most of the points (rules) in the scatter plots (and all rules for 50% of noise) are under the identity line, which means confidence and growth rate are wrongly optimistic and may lead to bad predictions.