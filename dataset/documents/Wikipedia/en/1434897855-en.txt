 In mathematics, a matrix (plural matrices) is a rectangular table of elements (or entries), which may be numbers or, more generally, any abstract quantities that can be added and multiplied. Matrices are used to describe linear equations, keep track of the coefficients of linear transformations and to record data that depend on multiple parameters. Matrices are described by the field of matrix theory. They can be added, multiplied, and decomposed in various ways, which also makes them a key concept in the field of linear algebra.In this article, the entries of a matrix are real or complex numbers unless otherwise noted.== Terminology and notation == The horizontal lines in a matrix are called rows and the vertical lines are called columns. A matrix with m'' rows and ''n columns is called an m''-by-''n matrix (written m''&nbsp;×&nbsp;''n) and m'' and ''n are called its dimensions. The dimensions of a matrix are always given with the number of rows first, then the number of columns. It is commonly said that an m''-by-''n matrix has an order of m''&nbsp;×&nbsp;''n ("order" meaning size). Two matrices of the same order whose corresponding entries are equivalent are considered equal.The entry that lies in the i''-th row and the ''j-th column of a matrix is typically referred to as the i,j, or (i,j), or (i,j)-th, or (i,j)th entry of the matrix. Again, the row is always noted first, then the column.Almost always upper-case letters denote matrices, while the corresponding lower-case letters, with two subscript indices, represent the entries. For example, the (i,j)th entry of a matrix A''' is most commonly written as ai,j. Alternative notations for that entry are '''A[i,j] or A'''i,j. In addition to using upper-case letters to symbolize matrices, many authors use a special typographical style, commonly boldface upright (non-italic), to further distinguish matrices from other variables. Following this convention, '''A is a matrix, distinguished from A'', a scalar. An alternate convention is to annotate matrices with their dimensions in small type underneath the symbol, for example, \underset for an ''m-by-''n'' matrix.We often write \mathbf:=(a_)_ or \mathbf:=(a_)_ to define an m'' × ''n matrix A. In this case, the entries ai,j are defined separately for all integers 1&nbsp;≤&nbsp;i''&nbsp;≤&nbsp;''m and 1&nbsp;≤&nbsp;j''&nbsp;≤&nbsp;''n. In some programming languages, the numbering of rows and columns starts at zero. Texts which use any such language extensively frequently follow that convention, so we have 0&nbsp;≤&nbsp;i''&nbsp;≤&nbsp;''m-1 and 0&nbsp;≤&nbsp;j''&nbsp;≤&nbsp;''n-1.A matrix where one of the dimensions equals one is often called a vector, and interpreted as an element of real coordinate space. An m''&nbsp;×&nbsp;1 matrix (one column and ''m rows) is called a column vector and a 1&nbsp;×&nbsp;n'' matrix (one row and ''n columns) is called a row vector.Mathematical definitionAn \,m \times n\,\,(m, n \in \mathbb) matrix \mathbf\, is a function  \mathbf\colon \ \times \ \to \mathbf,\,\, where \mathbf\, is any non-empty set.(\ \times \\, is the Cartesian product of sets \\, and \.)\,We say that matrix \mathbf is a matrix over the set \mathbf. Important thing to note is that, if we want to have matrix algebra, the set \mathbf\, must be a ring and matrix \mathbf must be a square matrix (see Square matrices and related definitions below for further explanation). Since the set of all square matrices over a ring is also a ring, matrix algebra is usually called matrix ring.Since this article mainly considers matrices over real numbers, matrices shown here are actually functions  \mathbf\colon \ \times \ \to \mathbb.\,\,ExampleThe matrix\mathbf = \begin9 & 8 & 6 \\ 1 & 2 & 7 \\ 4 & 9 & 2 \\ 6 & 0 & 5 \end &nbsp; or &nbsp; \mathbf = \begin 9 & 8 & 6 \\ 1 & 2 & 7 \\ 4 & 9 & 2 \\ 6 & 0 & 5 \endis a 4\times 3 matrix. The element a_ or \mathbf2,3 is 7. In terms of the mathematical definition given above, this matrix is a function  \mathbf\colon \ \times \ \to \mathbb\, and, for example,  \mathbf((2, 3)) = 7\, and  \mathbf((3, 1)) = 4.\,The matrix \mathbf = \begin 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \end is a 1\times 9 matrix, or 9-element row vector.Basic operationsSumTwo or more matrices of identical dimensions m'' and ''n can be added. Given m''-by-''n matrices A''' and '''B, their sum A+'B' is the m''-by-''n matrix computed by adding corresponding elements:\begin\mathbf+\mathbf &= (a_)_ + (b_)_\\ &= (a_+b_)_.\\ \end For example:\begin 1 & 3 & 1 \\ 1 & 0 & 0 \\ 1 & 2 & 2 \end + \begin 0 & 0 & 5 \\ 7 & 5 & 0 \\ 2 & 1 & 1 \end = \begin 1+0 & 3+0 & 1+5 \\ 1+7 & 0+5 & 0+0 \\ 1+2 & 2+1 & 2+1 \end = \begin 1 & 3 & 6 \\ 8 & 5 & 0 \\ 3 & 3 & 3 \end. Another, much less often used notion of matrix addition is the direct sum.Scalar multiplicationGiven a matrix A''' and a number c, the scalar multiplication ''cA' is computed by multiplying every element of A by the scalar c (i.e. (c\mathbf)_ = c \cdot a_). For example:2 \cdot\begin 1 & 8 & -3 \\ 4 & -2 & 5 \end = \begin 2 \cdot 1 & 2\cdot 8 & 2\cdot -3 \\ 2\cdot 4 & 2\cdot -2 & 2\cdot 5 \end = \begin 2 & 16 & -6 \\ 8 & -4 & 10 \end. Matrix addition and scalar multiplication turn the set \text(m,n,\mathbb) of all m-by-n matrices with real entries into a real vector space of dimension m\cdot n.Matrix multiplicationMultiplication of two matrices is well-defined only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A''' is an m''-by-''n matrix and '''B is an n''-by-''p matrix, then their matrix product AB is the m''-by-''p matrix given by:(\mathbf)_ = a_ b_ + a_ b_ + \ldots + a_ b_ for each pair (i,j). For example:\begin 1 & 0 & 2 \\1 & 3 & 1 \\\end \times \begin 3 & 1 \\ 2 & 1 \\ 1 & 0 \\ \end = \begin( 1 \times 3 + 0 \times 2 + 2 \times 1) & ( 1 \times 1 + 0 \times 1 + 2 \times 0) \\(-1 \times 3 + 3 \times 2 + 1 \times 1) & (-1 \times 1 + 3 \times 1 + 1 \times 0) \\\end = \begin 5 & 1 \\ 4 & 2 \\ \end. Matrix multiplication has the following properties:(AB)C''' = '''A(BC) for all k''-by-''m matrices A', ''m-by-''n'' matrices '''B and n''-by-''p matrices C ("associativity").(A'+'B)C''' = '''AC+'BC' for all m''-by-''n matrices A''' and '''B and n''-by-''k matrices C ("right distributivity").C'('A+'B') = CA+'CB' for all m''-by-''n matrices A''' and '''B and k''-by-''m matrices C ("left distributivity").Matrix multiplication is not commutative; that is, given matrices A''' and '''B and their product defined, then generally AB&nbsp;\ne&nbsp;BA. It may also happen that AB is defined but BA is not defined.Besides the ordinary matrix multiplication just described, there exist other operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product.Linear transformationsMatrices can conveniently represent linear transformations (also known as "linear maps") between finite-dimensional vector spaces. Let Rn'' be an ''n-dimensional vector space, and let the vectors in this space be represented in matrix format as column vectors (n-by-1 matrices). For every linear map f'' : '''R'n'' → '''R'm'' there exists a unique ''m-by-''n'' matrix A such thatf(\mathbf x) = \mathbf for each vector x''' in '''Rn.We say that the matrix A''' "represents" the linear map f, or that '''A is the "transformation matrix" of f.Matrix multiplication neatly corresponds to the composition of maps. If the k''-by-''m matrix B''' represents another linear map g : '''Rm'' → '''R'k'', then the composition ''g&nbsp;o&nbsp;f'' is represented by '''BA':(g \circ f) (\mathbf x) = g (f(\mathbf x)) = g(\mathbf) = \mathbf B (\mathbf ) = (\mathbf ) \mathbf x \,.This follows from the above-mentioned associativity of matrix multiplication.More generally, a linear map from an n''-dimensional vector space to an ''m-dimensional vector space is represented by an m''-by-''n matrix, provided that bases have been chosen for each. This property makes matrices powerful data structures in high-level programming languages.Ranks The rank of a matrix A''' is the dimension of the image of the linear map represented by '''A; this is the same as the dimension of the space generated by the rows of A''', and also the same as the dimension of the space generated by the columns of '''A. It can also be defined without reference to linear algebra as follows: the rank of an m''-by-''n matrix A''' is the smallest number k such that '''A can be written as a product BC where B''' is an m''-by-''k matrix and '''C is a k''-by-''n matrix (although this is not a practical way to compute the rank).Transpose The transpose of an m''-by-''n matrix A''' is the n''-by-''m matrix '''AT (also sometimes written as A'tr, or t'A, or A') formed by turning rows into columns and columns into rows, i.e., A'T[''i, j] = '''A[j'', ''i] for all indices i'' and ''j. If A''' describes a linear map with respect to two bases, then the matrix '''AT describes the transpose of the linear map with respect to the dual bases, see dual space.We have (A''' + '''B)T = A'''T + '''BT and (AB)T = B'''T '''AT.==Square matrices and related definitions== A square matrix is a matrix which has the same number of rows and columns. The set of all square n''-by-''n matrices, together with matrix addition and matrix multiplication is a ring. Unless n = 1, this ring is not commutative.M(n'', '''R'), the ring of real square matrices, is a real unitary associative algebra. M(n'', '''C'), the ring of complex square matrices, is a complex associative algebra.The unit matrix or identity matrix In of size n'' is the ''n-by-''n'' matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0. The identity matrix is named thus because it satisfies the equations MIn''&nbsp;=&nbsp;'M''' and I'''n'''N&nbsp;=&nbsp;N''' for any m''-by-''n matrix '''M and n''-by-''k matrix N. For example, if n = 3:\mathbf_3 = \begin 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end . The identity matrix is the identity element in the ring of square matrices.Invertible elements in this ring are called invertible matrices or non-singular matrices. An n''-by-''n matrix A''' is invertible if and only if there exists a matrix '''B such thatAB = I'''n ( = '''BA).In this case, B''' is the '''inverse matrix of A''', denoted by '''A−1. The set of all invertible n''-by-''n matrices forms a group (specifically a Lie group) under matrix multiplication, the general linear group.If λ is a number and v''' is a non-zero vector such that '''Av = λ'v', then we call v''' an eigenvector of '''A and λ the associated eigenvalue. (Eigen means "own" in German and in Dutch.) The number λ is an eigenvalue of A''' if and only if '''A−λ'I'n'' is not invertible, which happens if and only if ''pA'(λ) = 0, where ''p'''A(x'') is the characteristic polynomial of '''A'. p'''A'(''x) is a polynomial of degree n'' and therefore has ''n complex roots if multiple roots are counted according to their multiplicity. Every square matrix has at most n complex eigenvalues.The determinant of a square matrix A is the product of its n'' eigenvalues, but it can also be defined by the ''Leibniz formula. Invertible matrices are precisely those matrices with a nonzero determinant.The Gaussian elimination algorithm is of central importance: it can be used to compute determinants, ranks and inverses of matrices and to solve systems of linear equations.The trace of a square matrix is the sum of its diagonal entries, which equals the sum of its n eigenvalues.The square root of a matrix A''' is any matrix '''B such that A''' = '''B*'B', where B'''* denotes the conjugate transpose of '''B. The matrix exponential function is defined for square matrices, using power series.Special types of matricesIn many areas in mathematics, matrices with certain structure arise. A few important examples areSymmetric matrices are such that entries symmetric about the main diagonal (from the upper left to the lower right) are equal, that is, expressed entry-wise, a_=a_, or equivalently as a matrix equation, \mathbf^\mathrm = \mathbf.Skew-symmetric matrices are such that entries symmetric about the main diagonal are the negative of each other, that is, a_=-a_, or equivalently \mathbf^\mathrm=-\mathbf. In a skew-symmetric matrix, all diagonal entries are zero, that is, a_=-a_\Rightarrow a_=0.Hermitian (or self-adjoint) matrices are such that entries symmetric about the diagonal are each others complex conjugates, that is, a_=\overline_, or equivalently \mathbf^\ast = \mathbf, where \overline signifies the complex conjugate of a complex number z and \,\! \mathbf^\ast the conjugate transpose of A.Toeplitz matrices have common entries on their diagonals, that is, \,\! a_=a_.Stochastic matrices are square matrices whose rows are probability vectors; they are used to define Markov chains.A square matrix A is called idempotent if \mathbf^2=\mathbf=\mathbf.For a more extensive list see list of matrices.Matrices in abstract algebraIf we start with a ring R'', we can consider the set M(''m,n'', ''R) of all m'' by ''n matrices with entries in R''. Addition and multiplication of these matrices can be defined as in the case of real or complex matrices (see above). The set M(''n, R'') of all square ''n by n'' matrices over ''R is a ring in its own right, isomorphic to the endomorphism ring of the left R''-module ''Rn.Similarly, if the entries are taken from a semiring S'', matrix addition and multiplication can still be defined as usual. The set of all square ''n×''n'' matrices over S is itself a semiring. Note that fast matrix multiplication algorithms such as the Strassen algorithm generally only apply to matrices over rings and will not work for matrices over semirings that are not rings.If R'' is a commutative ring, then M(''n, R'') is a unitary associative algebra over ''R. It is then also meaningful to define the determinant of square matrices using the Leibniz formula; a matrix is invertible if and only if its determinant is invertible in R.All statements mentioned in this article for real or complex matrices remain correct for matrices over an arbitrary field.Matrices over a polynomial ring are important in the study of control theory.== Matrices without entries == A subtle question that is hardly ever posed is whether there is such a thing as a 3-by-0 matrix. That would be a matrix with 3 rows but without any columns, which seems absurd. However, if one wants to be able to have matrices for all linear maps between finite dimensional vector spaces, one needs such matrices, since there is nothing wrong with linear maps from a 0-dimensional space to a 3-dimensional space (in fact if the spaces are fixed there is one such map, the zero map). So one is led to admit that there is exactly one 3-by-0 matrix (which has 3&times;0=0 entries; not null entries but none at all). Similarly there are matrices with a positive number of columns but no rows.Even in absence of entries, one must still keep track of the number of rows and columns, since the product BC where B''' is the 3-by-0 matrix and '''C is a 0-by-4 matrix is a perfectly normal 3-by-4 matrix, all of whose 12 entries are 0 (as they are given by an empty sum). Note that this computation of BC justifies the criterion given above for the rank of a matrix in terms of possible expressions as a product: the 3-by-4 matrix with zero entries certainly has rank 0, so it should be the product of a 3-by-0 matrix and a 0-by-4 matrix. . . To allow and distinguish between matrices without entries, matrices should formally be defined, in a somewhat pedantic computer science style, as quadruples (A'', ''r, c'', ''M), where A'' is the set in which the entries live, ''r and c'' are the (natural) numbers of rows and columns, and ''M is the rectangular collection of rc elements of A (the matrix in the usual sense).HistoryThe study of matrices is quite old. A 3-by-3 magic square appears in the Chinese literature dating from as early as 650 BC.Swaney, Mark. History of Magic Squares.Matrices have a long history of application in solving linear equations. An important Chinese text from between 300 BC and AD 200, The Nine Chapters on the Mathematical Art (Jiu Zhang Suan Shu), is the first example of the use of matrix methods to solve simultaneous equations. cited by  In the seventh chapter, "Too much and not enough," the concept of a determinant first appears almost 2000 years before its publication by the Japanese mathematician Seki Kowa in 1683 and the German mathematician Gottfried Leibniz in 1693.Magic squares were known to Arab mathematicians, possibly as early as the 7th century, when the Arabs conquered northwestern parts of the Indian subcontinent and learned Indian mathematics and astronomy, including other aspects of combinatorial mathematics. It has also been suggested that the idea came via China. The first magic squares of order 5 and 6 appear in an encyclopedia from Baghdad circa 983 AD, the Encyclopedia of the Brethren of Purity (Rasa'il Ihkwan al-Safa); simpler magic squares were known to several earlier Arab mathematicians.The concept of matrix as we know it started with linear algebra. Later, after the development of the theory of determinants by Seki Kowa and Leibniz in the late 17th century, Cramer developed the theory further in the 18th century, presenting Cramer's rule in 1750. Carl Friedrich Gauss and Wilhelm Jordan developed Gauss-Jordan elimination in the 1800s.The term "matrix" was coined in 1848 by J. J. Sylvester. Cayley, Hamilton, Grassmann, Frobenius and von Neumann are among the famous mathematicians who have worked on matrix theory.Olga Taussky-Todd (1906-1995) made important contributions to matrix theory, using it to investigate an aerodynamic phenomenon called fluttering or aeroelasticity during World War II. She has been called "a torchbearer" for matrix theory.Ivars Peterson. Matrices, Circles, and Eigenthings.EducationMatrices were traditionally taught as part of linear algebra in college, or with calculus. With the adoption of integrated mathematics texts for use in high school in the United States in the 1990s, they have been included by many such texts such as the Core-Plus Mathematics Project which are often targeted as early as the ninth grade, or earlier for honors students. They often require the use of graphing calculators such as the TI-83 which can perform complex operations such as matrix inversion very quickly.Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL, were designed to manipulate matrices, and mathematical programs such as Mathematica, and others are used to aid computing with matrices.ApplicationsEncryptionMatrices can be used to encrypt numerical data. Encryption is done by multiplying the data matrix with a key matrix. Decryption is done simply by multiplying the encrypted matrix with the inverse of the key.Computer graphics4×4 transformation matrices are commonly used in computer graphics. The upper left 3×3 portion of a transformation matrix is composed of the new X'', ''Y, and Z axes of the post-transformation coordinate space.Further readingA more advanced article on matrices is Matrix theory.See alsoDeterminantList of matricesMatrix decompositionLogical matrixMatrix calculusReferencesExternal links ResourcesMatrix name and history: very brief overview, ualr.eduIntroduction to Matrix Algebra: definitions and properties, xycoon.comMatrix Algebra, sosmath.comThe Matrix Reference Manual, Imperial CollegeApplied examples of matrices used in graphical game programming, Riemer's DirectX TutorialsAn Introduction to Matrix Algebra by Autar Kaw, A simple primer for a beginner or one who is rusty on the topicOnline Matrix Calculatorseasycalculation.comMatri-tri-ca - Matrix Calculatorbluebit.grwims.unice.frMatrix Calculator at SolveMyMath - Calculate the Determinant, Trace, Transpose and Inverse Matrix of a MatrixFreewareMATRIX 2.1 Excel add-in, foxesMacAnova, University of Minnesota School of StatisticsCategory:Abstract algebra Category:Linear algebra Matrices ar:مصفوفة az:Matris bn:মেট্রিক্স bs:Matrica (matematika) bg:Матрица (математика) ca:Matriu (matemàtiques) cs:Matice da:Matrix de:Matrix (Mathematik) et:Maatriks es:Matriz (matemática) eo:Matrico fa:ماتریس (ریاضی) fr:Matrice (mathématiques) gl:Matriz (matemáticas) ko:행렬 hr:Matrica (matematika) id:Matriks (matematika) is:Fylki (stærðfræði) it:Matrice he:מטריצה lo:ມາຕຣິກ lt:Matrica (matematika) hu:Mátrix (matematika) mk:Матрица ml:മാട്രിക്സ് ms:Matriks (matematik) nl:Matrix (wiskunde) ja:行列 no:Matrise nn:Matrise pl:Macierz pt:Matriz (matemática) ro:Matrice (matematică) ru:Матрица (математика) sq:Matrica simple:Matrix (mathematics) sk:Matica (matematika) sl:Matrika sr:Матрица (математика) fi:Matriisi sv:Matris ta:அணி th:เมทริกซ์ (คณิตศาสตร์) vi:Ma trận (toán học) tr:Dizey uk:Матриця (математика) ur:میٹرکس zh:矩阵